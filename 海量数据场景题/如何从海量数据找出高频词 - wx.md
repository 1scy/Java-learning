本期是【**大厂面试**】系列文章的第**26**期

回复【**手册**】获取大彬精心整理的**大厂面试手册**。

## 面试开始

旁白：通过微信聊天方式模拟现场面试，题目出自腾讯PCG一面

**面试官**：**今天来道场景题**

**面试官**：假如有一个1G大小的文件，文件里每一行是一个词，每个词的大小不超过16byte，要求返回出现频率最高的100个词

**面试官**：内存大小限制是10M

旁白：由于内存限制，我们无法直接将大文件的所有词一次性读到内存中

旁白：可以采用**分治策略**，把一个大文件分解成多个小文件，保证每个文件的大小小于10M，进而直接将单个小文件读取到内存中进行处理

**大彬**：第一步，首先遍历大文件，对遍历到的每个词x，执行 `hash(x) % 500`，将结果为i的词存放到文件f(i)中

**大彬**：遍历结束后，可以得到500个小文件

**大彬**：之所以使用500个小文件，是因为原文件大小为1G，1G/500=2M，每个小文件的大小为2M左右，基本不会超过内存大小10M的限制

**大彬**：第二步，接着统计每个小文件中出现频数最高的100个词

**大彬**：可以使用HashMap来实现，其中key为词，value为该词出现的频率

**大彬**：对于遍历到的词x，如果在map中不存在，则执行 `map.put(x, 1)`

**大彬**：若存在，则执行 `map.put(x, map.get(x)+1)`，将该词出现的次数加1

**大彬**：第三步，在第二步中找出了每个文件出现频率最高的100个词之后，通过维护一个小顶堆来找出所有小文件中出现频率最高的100个词

**大彬**：具体方法是，遍历第一个文件，把第一个文件中出现频率最高的100个词构建成一个小顶堆

**大彬**：如果第一个文件中词的个数小于100，可以继续遍历第二个文件，直到构建好有100个结点的小顶堆为止

**大彬**：继续遍历其他小文件，如果遍历到的词的出现次数大于堆顶上词的出现次数，可以用新遍历到的词替换堆顶的词，然后重新调整这个堆为小顶堆

**大彬**：当遍历完所有小文件后，这个小顶堆中的词就是出现频率最高的100个词

**大彬**：最后总结一下

1. 采用**分治**的思想，进行哈希取余
2. 使用**HashMap**统计每个小文件单词出现的次数
3. 使用**小顶堆**，遍历步骤2中的小文件，找出出现频率Top100的单词

面试官：在第二步中，将1G的文件分解到500个小文件，小文件的大小可能超过10M吧？怎么处理？

大彬：如果某个小文件的大小超过10MB了，可以采用相同的方法对这个文件继续分解成g(1)...g(x)，直到文件的大小小于10MB为止

**面试官**：good！今天面试就到这吧



## 点关注，不迷路

大彬，**非科班出身，自学Java**，校招斩获京东、携程、华为等offer。作为一名转码选手，深感这一路的不易。

希望我的分享能帮助到更多的小伙伴，**我踩过的坑你们不要再踩**。想与大彬交流的话，可以到公众号后台获取大彬的微信~

后台回复『 **笔记**』即可领取大彬斩获大厂offer的**面试笔记**。

![](https://raw.githubusercontent.com/Tyson0314/img/master/公众号.jpg)

